{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and extracting the Fashionpedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a git filter that strips the notebook output when committing\n",
    "!git config filter.strip-notebook-output.clean 'jupyter nbconvert --ClearOutputPreprocessor.enabled=True --to=notebook --stdin --stdout --log-level=ERROR'\n",
    "\n",
    "# Download dataset (4 GB)\n",
    "!mkdir fashionpedia\n",
    "!mkdir fashionpedia/img\n",
    "\n",
    "!curl https://s3.amazonaws.com/ifashionist-dataset/images/train2020.zip -o fashionpedia/train.zip\n",
    "!unzip fashionpedia/train.zip -d fashionpedia/img\n",
    "!rm fashionpedia/train.zip\n",
    "\n",
    "!curl https://s3.amazonaws.com/ifashionist-dataset/annotations/instances_attributes_train2020.json -o fashionpedia/attributes.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lib.fashionpedia_type as fpt\n",
    "\n",
    "with open('fashionpedia/attributes.json') as item:\n",
    "    att: fpt.FashionPedia = json.load(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_by_id(group, id):\n",
    "    '''Helper function for finding a single item by id from a group in the dataset'''\n",
    "    for item in att[group]:\n",
    "        if item['id'] == id:\n",
    "            return item"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_segmentation(annotation: fpt.Annotation):\n",
    "    file_name = get_by_id('images', annotation['image_id'])['file_name']\n",
    "    img = mpimg.imread(f'fashionpedia/img/{file_name}')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    seg = annotation['segmentation'][0]\n",
    "    xs = seg[::2]\n",
    "    ys = seg[1::2]\n",
    "    plt.plot(xs, ys, c='red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "test = get_by_id('annotations', 11)\n",
    "print(test['area'])\n",
    "show_segmentation(test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def crop_segmentation(annotation: fpt.Annotation):\n",
    "    file_name = get_by_id('images', annotation['image_id'])['file_name']\n",
    "    img = Image.open(f'fashionpedia/img/{file_name}').convert('RGB')\n",
    "\n",
    "    # make segmentation mask\n",
    "    mask = Image.new('1', img.size, 1)\n",
    "\n",
    "    seg = annotation['segmentation'][0]\n",
    "    points = list(zip(*(iter(seg),) * 2))\n",
    "\n",
    "    ImageDraw.Draw(mask).polygon(points, outline=0, fill=0)\n",
    "    \n",
    "    # Set all but masked area white\n",
    "    img.paste((256, 256, 256), mask)\n",
    "    \n",
    "    # crop to bounding box\n",
    "    x, y, width, height = map(int, annotation['bbox'])\n",
    "    cropped_image = img.crop((x, y, x + width, y + height))\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "test = get_by_id('annotations', 11)\n",
    "plt.imshow(crop_segmentation(test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def resize(img):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224))\n",
    "    ])\n",
    "\n",
    "    return transform(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample data from original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset sampling conditions\n",
    "SUPERCATEGORY = 'upperbody'\n",
    "DATASET_SIZE = 5000\n",
    "MIN_GARMENT_AREA = 1\n",
    "\n",
    "selected_cat = [cat for cat in att['categories'] if cat['supercategory'] == SUPERCATEGORY]\n",
    "selected_cat_ids = [cat['id'] for cat in selected_cat]\n",
    "\n",
    "print(\"Selected categories:\", *[cat['name'] for cat in selected_cat], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# select 32 most frequent attributes\n",
    "attribute_ids = [att_id for a in att['annotations'] if a['category_id'] in selected_cat_ids for att_id in a['attribute_ids']]\n",
    "attribute_freqs = np.unique(attribute_ids, return_counts=True)\n",
    "selected_att_ids, _ = list(zip(*sorted(zip(*attribute_freqs), key=lambda i: i[1], reverse=True)[:32]))\n",
    "sorted_selected_att_ids = sorted(selected_att_ids)\n",
    "\n",
    "print(\"Most frequent attributes:\", *[get_by_id('attributes', att_id)['name'] for att_id in selected_att_ids], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "dataset = []\n",
    "att_counts = {int(i): 0 for i in selected_att_ids}\n",
    "with tqdm(total=DATASET_SIZE) as pbar:\n",
    "    for ann in att['annotations']:\n",
    "        if not ann['category_id'] in selected_cat_ids:\n",
    "            continue\n",
    "\n",
    "        if ann['area'] < MIN_GARMENT_AREA:\n",
    "            continue\n",
    "\n",
    "        if type(ann['segmentation']) != list:\n",
    "            # skip images with RLE segmentation masks\n",
    "            continue\n",
    "\n",
    "        attributes = [att_id for att_id in ann['attribute_ids'] if att_id in selected_att_ids]\n",
    "        attributes_one_hot = [int(att_id in attributes) for att_id in sorted_selected_att_ids]\n",
    "\n",
    "        if not attributes:\n",
    "            continue\n",
    "\n",
    "        for a in attributes:\n",
    "            att_counts[a] += 1\n",
    "\n",
    "        # Crop, set to tensor and resize image\n",
    "        img = crop_segmentation(ann)\n",
    "        img = resize(img)\n",
    "        \n",
    "        dataset.append({\n",
    "                'fn': get_by_id('images', ann['image_id'])['file_name'],\n",
    "                'img': img,\n",
    "                'cat': ann['category_id'],\n",
    "                'att_oh': torch.tensor(attributes_one_hot, dtype=torch.float)\n",
    "            })\n",
    "        pbar.update()\n",
    "        \n",
    "        if len(dataset) >= DATASET_SIZE:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(data):\n",
    "\t# Transform list to a Tensor\n",
    "\timages = torch.stack([d['img'] for d in data])\n",
    "\n",
    "\t# Create normalization transformation\n",
    "\tmeans = torch.mean(images, dim = [0,2,3])\n",
    "\tstds = torch.std(images, dim = [0,2,3])\n",
    "\tnormalize = transforms.Normalize(mean=means, std=stds, inplace=True)\n",
    "\n",
    "\t# Normalize the images\n",
    "\tfor d in tqdm(data):\n",
    "\t\tnormalize(d['img'])\n",
    "\n",
    "\treturn means, stds\n",
    "\n",
    "\n",
    "means, stds = normalize_dataset(dataset)\n",
    "plt.imshow(dataset[2]['img'].permute(1, 2, 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# save dataset\n",
    "with open(\"fashionpedia/processed_dataset.pt\", \"wb\") as file:\n",
    "    torch.save({\"dataset\": dataset, \"means\": means, \"stds\": stds}, file)\n",
    "\n",
    "with open(\"fashionpedia/selected_attributes.json\", \"w\") as file:\n",
    "    selected_atts = {int(i): get_by_id('attributes', i)['name'] for i in selected_att_ids}\n",
    "\n",
    "    json.dump(selected_atts, file, indent=1, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh([get_by_id('attributes', i)['name'] for i in att_counts.keys()], att_counts.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_attributes(img):\n",
    "    i_id = img['id']\n",
    "    image_id = img['image_id']\n",
    "\n",
    "    style_annotations = get_by_id('annotations', i_id)\n",
    "    style_image = get_by_id('images', image_id)\n",
    "\n",
    "    cat_names = att['categories'][style_annotations['category_id']]['name']\n",
    "    att_names = ', '.join([att_['name']  for attri_id in style_annotations['attribute_ids'] for att_ in att['attributes'] if att_['id'] == attri_id]).replace('\\\\'', '')\n",
    "\t\n",
    "    prompt = f'Categories: {cat_names}, Attributes: {att_names}'\n",
    "\n",
    "    print(prompt)\n",
    "    print(style_image['file_name'])\n",
    "    print(style_image['original_url'])\n",
    "    print('\\\\n')\n",
    "\n",
    "    return style_annotations\n",
    "    \n",
    "for d in dataset[:5]:\n",
    "    test = get_image_and_attributes(d)\n",
    "    plt.imshow(crop_segmentation(test))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
