{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and extracting the Fashionpedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a git filter that strips the notebook output when committing\n",
    "!git config filter.strip-notebook-output.clean 'jupyter nbconvert --ClearOutputPreprocessor.enabled=True --to=notebook --stdin --stdout --log-level=ERROR'\n",
    "\n",
    "# Download dataset (4 GB)\n",
    "!mkdir fashionpedia\n",
    "!mkdir fashionpedia/img\n",
    "\n",
    "!curl https://s3.amazonaws.com/ifashionist-dataset/images/train2020.zip -o fashionpedia/train.zip\n",
    "!unzip fashionpedia/train.zip -d fashionpedia/img\n",
    "!rm fashionpedia/train.zip\n",
    "\n",
    "!curl https://s3.amazonaws.com/ifashionist-dataset/annotations/instances_attributes_train2020.json -o fashionpedia/attributes.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import lib.fashionpedia_type as fpt\n",
    "from tqdm.autonotebook import tqdm\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "\n",
    "# Dataset sampling conditions\n",
    "SUPERCATEGORY = 'upperbody'\n",
    "DATASET_SIZE = 100\n",
    "MIN_GARMENT_AREA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fashionpedia/attributes.json') as item:\n",
    "    att: fpt.FashionPedia = json.load(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cat = [cat for cat in att['categories'] if cat['supercategory'] == SUPERCATEGORY]\n",
    "selected_cat_ids = [cat['id'] for cat in selected_cat]\n",
    "\n",
    "print(\"Selected categories:\", [cat['name'] for cat in selected_cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_by_id(group, id):\n",
    "    for item in att[group]:\n",
    "        if item['id'] == id:\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_segmentation(annotation: fpt.Annotation):\n",
    "    file_name = get_by_id('images', annotation['image_id'])['file_name']\n",
    "    img = mpimg.imread(f'fashionpedia/img/train/{file_name}') # Too much trouble to move all the images so let's work with two location :)\n",
    "    # img = mpimg.imread(f'fashionpedia/img/{file_name}')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    seg = annotation['segmentation'][0]\n",
    "    xs = seg[::2]\n",
    "    ys = seg[1::2]\n",
    "    plt.plot(xs, ys, c='red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "test = get_by_id('annotations', 11)\n",
    "print(test['area'])\n",
    "show_segmentation(test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_colour(img):\n",
    "\tNUM_CLUSTERS = 5\n",
    "\timg = img.resize((150, 150)) # For speed\n",
    "\n",
    "\tar = np.asarray(img)\n",
    "\tshape = ar.shape\n",
    "\tar = ar.reshape(scipy.product(shape[:2]), shape[2]).astype(float)\n",
    "\n",
    "\tcodes, _ = scipy.cluster.vq.kmeans(ar, NUM_CLUSTERS)\n",
    "\tvecs, _ = scipy.cluster.vq.vq(ar, codes)         # assign codes\n",
    "\tcounts, _ = scipy.histogram(vecs, len(codes))    # count occurrences\n",
    "\n",
    "\tindex_max = scipy.argmax(counts)                    # find most frequent\n",
    "\tpeak = codes[index_max]\n",
    "\tcolour = list(int(c) for c in peak)\n",
    "\n",
    "\treturn colour\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_segmentation(annotation: fpt.Annotation):\n",
    "    file_name = get_by_id('images', annotation['image_id'])['file_name']\n",
    "    img = Image.open(f'fashionpedia/img/train/{file_name}') # Too much trouble to move all the images so let's work with two location :)\n",
    "    # img = Image.open(f'fashionpedia/img/{file_name}')\n",
    "\n",
    "    mask = Image.new('1', img.size, 0)\n",
    "    inv_mask = Image.new('1', img.size, 1)\n",
    "\n",
    "    # Mask\n",
    "    seg = annotation['segmentation'][0]\n",
    "    points = list(zip(*(iter(seg),) * 2))\n",
    "\n",
    "    ImageDraw.Draw(mask).polygon(points, outline=1, fill=1)\n",
    "    ImageDraw.Draw(inv_mask).polygon(points, outline=1, fill=0)\n",
    "\n",
    "    # TODO: this makes all masked pixels transparent, but their color value is still the same, so the information is still there. We should make the masked pixels black or white first\n",
    "    \n",
    "    # crop to bounding box\n",
    "    x, y, width, height = map(int, annotation['bbox'])\n",
    "    cropped_image_dominant = img.crop((x, y, x + width, y + height))\n",
    "\n",
    "    dominant_colour = get_dominant_colour(cropped_image_dominant)\n",
    "\n",
    "    # If we have a high chroma image we set background colour black otherwise we set it to white\n",
    "    if (dominant_colour[0] >= 175 and dominant_colour[1] >= 175 and dominant_colour[2] <= 75) or \\\n",
    "        (dominant_colour[0] <= 75 and dominant_colour[1] >= 175 and dominant_colour[2] <= 75) or \\\n",
    "        (dominant_colour[0] <= 75 and dominant_colour[1] >= 175 and dominant_colour[2] >= 175) or \\\n",
    "        (dominant_colour[0] >= 175 and dominant_colour[1] <= 75 and dominant_colour[2] >= 175) or \\\n",
    "        (dominant_colour[0] >= 175 and dominant_colour[1] >= 175 and dominant_colour[2] >= 175):\n",
    "        # Set background black\n",
    "        img.paste((0, 0, 0), inv_mask)\n",
    "    else:\n",
    "        # Set background white\n",
    "        img.paste((256, 256, 256), inv_mask)\n",
    "\n",
    "    # img.putalpha(mask)\n",
    "\n",
    "    # crop to bounding box\n",
    "    # x, y, width, height = map(int, annotation['bbox'])\n",
    "    cropped_image = img.crop((x, y, x + width, y + height))\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "test = get_by_id('annotations', 11)\n",
    "plt.imshow(crop_segmentation(test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "SELECTED_CATEGORIES = [6] # cat 6 is pants\n",
    "SELECTED_ATTRIBUTES = np.arange(start=36, stop=50, step=1) # att 36 - 49 are pants attributes like: [jeans, cargo, sweat, camo, ...]\n",
    "\n",
    "\n",
    "for ann in att['annotations']:\n",
    "    # if not ann['category_id'] in selected_cat_ids:\n",
    "    #     continue\n",
    "\n",
    "    if ann['area'] < MIN_GARMENT_AREA:\n",
    "        continue\n",
    "\n",
    "    if type(ann['segmentation']) != list:\n",
    "        # skip images with RLE segmentation masks\n",
    "        continue\n",
    "\n",
    "    if ann['category_id'] not in SELECTED_CATEGORIES:\n",
    "        continue\n",
    "\n",
    "    if len([i for i in ann['attribute_ids'] if i in SELECTED_ATTRIBUTES]) == 0:\n",
    "        continue\n",
    "\n",
    "    img = crop_segmentation(ann)\n",
    "    # plt.imshow(img)\n",
    "    # plt.show()\n",
    "\n",
    "    dataset.append({\n",
    "            'fn': get_by_id('images', ann['image_id'])['file_name'],\n",
    "            'img': img,\n",
    "            'cat': ann['category_id'],\n",
    "            'att': ann['attribute_ids'],\n",
    "        })\n",
    "    \n",
    "    if len(dataset) >= DATASET_SIZE:\n",
    "        break\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO save preprocessed dataset\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "with open(\"fashionpedia/processed_dataset.pkl\", \"wb\") as file:\n",
    "    pkl.dump(dataset, file)\n",
    "    \n",
    "with open(\"fashionpedia/processed_dataset.pkl\", \"rb\") as file:\n",
    "    dataset2 = pkl.load(file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all attributes\n",
    "set(attrib['name'] for attrib in att['attributes'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
