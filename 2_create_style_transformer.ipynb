{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from models.vgg19style import VGG19Style\n",
    "from lib.fashionpedia_processed import FashionPediaProcessed\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else device\n",
    "print(device)\n",
    "\n",
    "model = VGG19Style().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = FashionPediaProcessed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import lib.utils as utils\n",
    "import torchvision.transforms as T\n",
    "\n",
    "style_data = data[66]\n",
    "content_data = data[18]\n",
    "\n",
    "plt.imshow(data.invImg(content_data['img']))\n",
    "plt.title('Content image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print('attributes:', *[utils.get_attribute_name(att_id) for att_id in content_data['atts']], sep='\\n')\n",
    "\n",
    "plt.imshow(data.invImg(style_data['img']))\n",
    "plt.title('Style image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print('attributes:', *[utils.get_attribute_name(att_id) for att_id in style_data['atts']], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor: torch.Tensor):\n",
    "    depth = tensor.shape[0]\n",
    "    tensor = tensor.view(depth, -1)\n",
    "    return torch.mm(tensor, tensor.t()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the activations of the selected layers\n",
    "\n",
    "activations = {}\n",
    "def set_activation(name):\n",
    "    return lambda _, __, output: activations.update({name: output})\n",
    "\n",
    "# content layer\n",
    "model.layers[21].register_forward_hook(set_activation('conv_4_2'))\n",
    "\n",
    "# style layers\n",
    "style_layers = {0: 'conv_1_1', 5: 'conv_2_1', 10: 'conv_3_1', 19: 'conv_4_1', 28: 'conv_5_1'}\n",
    "for i, name in style_layers.items():\n",
    "    model.layers[i].register_forward_hook(set_activation(name))\n",
    "\n",
    "# save activations for content and style reference imgs\n",
    "model(content_data['img'].to(device))\n",
    "content_img_feature = activations['conv_4_2']\n",
    "\n",
    "model(style_data['img'].to(device))\n",
    "style_img_grams = {i: gram_matrix(activations[name]) for i, name in style_layers.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_content_loss(content_img_feature, target_img_feature):\n",
    "\treturn torch.sum((content_img_feature - target_img_feature) ** 2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(style_img_grams, target_img_features):\n",
    "\tloss = 0\n",
    "\n",
    "\tfor i in style_layers.keys():\n",
    "\t\ttarget_gram = gram_matrix(target_img_features[i])\n",
    "\t\tsquared_err = torch.sum((target_gram - style_img_grams[i]) ** 2)\n",
    "\n",
    "\t\t_, height, width = target_img_features[i].shape\n",
    "\t\tloss += squared_err / (4 * height ** 2 * width ** 2)\n",
    "\n",
    "\treturn loss / len(style_layers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single image content transfer from noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate target image\n",
    "target_img = torch.normal(0, 1, content_data['img'].shape, requires_grad=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import trange\n",
    "\n",
    "# gaussian noise image\n",
    "optimizer = torch.optim.Adam([target_img], lr=0.001)\n",
    "iterations = 50000\n",
    "\n",
    "for i in trange(iterations + 1):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model(target_img)\n",
    "\n",
    "    target_content_feature = activations['conv_4_2']\n",
    "\n",
    "    content_loss = get_content_loss(content_img_feature, target_content_feature)\n",
    "\n",
    "    content_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        plt.title(f'Content, iteration: {i}, Loss: {content_loss.item()}')\n",
    "        plt.imshow(target_img.cpu().detach().permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f'blog/imgs/noise_content/{i}')\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single image style transfer from noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img = torch.normal(0, 1, content_data['img'].shape, requires_grad=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import trange\n",
    "\n",
    "optimizer = torch.optim.Adam([target_img], lr=0.001)\n",
    "iterations = 50000\n",
    "\n",
    "for i in trange(iterations + 1):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model(target_img)\n",
    "\n",
    "    target_style_features = {i: activations[name] for i, name in style_layers.items()}\n",
    "\n",
    "    style_loss = get_style_loss(style_img_grams, target_style_features)\n",
    "\n",
    "    style_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        plt.title(f'Style, iteration: {i}, loss: {round(style_loss.item(), 4)}')\n",
    "        plt.imshow(target_img.cpu().detach().permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f'blog/imgs/style_content/{i}')\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General style transfer on a content image\n",
    "\n",
    "By using multiple images with the same style might generalize a style on the content image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all images with certain style \n",
    "\n",
    "selected_att = 322 # checkered\n",
    "# selected_att = 321 # camouflage\n",
    "# selected_att = 325 # floral\n",
    "\n",
    "style_images = []\n",
    "for i, d in enumerate(data): \n",
    "\tif selected_att in d['atts']:\n",
    "\t\tstyle_images.append(d['img'])\n",
    "\n",
    "print(f'Number of style images: {len(style_images)}')\n",
    "fig, axes = plt.subplots(1, 10)\n",
    "# axes[2].set_title(f'Style images: {utils.get_attribute_name(selected_att)}')\n",
    "\n",
    "for i, img in enumerate(style_images[:10]):\n",
    "\taxes[i].imshow(data.invImg(img))\n",
    "\taxes[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set content and style image features\n",
    "content_image = content_data['img']\n",
    "\n",
    "model(content_image.to(device))\n",
    "\n",
    "content_img_feature = activations['conv_4_2']\n",
    "\n",
    "style_images_grams = []\n",
    "\n",
    "for style_image in tqdm(style_images):\n",
    "\tmodel(style_image.to(device))\n",
    "\n",
    "\tstyle_images_grams.append({i: gram_matrix(activations[name]) for i, name in style_layers.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import trange\n",
    "\n",
    "# Set constants\n",
    "ALPHA_BETA = 1e-3\n",
    "ITERATIONS = 3000\n",
    "LR = 1e-3\n",
    "\n",
    "target_img = torch.tensor(content_image, device=device, requires_grad=True)\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = torch.optim.Adam([target_img], lr=LR)\n",
    "\n",
    "for i in trange(ITERATIONS):\n",
    "\n",
    "    for style_img_gram in style_images_grams[:10]:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model(target_img)\n",
    "\n",
    "        target_content_feature = activations['conv_4_2']\n",
    "\n",
    "        content_loss = get_content_loss(content_img_feature, target_content_feature)\n",
    "\n",
    "        target_style_features = {k: activations[name] for k, name in style_layers.items()}\n",
    "\n",
    "        style_loss = get_style_loss(style_img_gram, target_style_features)\n",
    "\n",
    "        total_loss = ALPHA_BETA * content_loss + style_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        plt.title(f'Iterations: {i}, Loss: {total_loss.item()}')\n",
    "        plt.imshow(data.invImg(target_img.cpu().detach()))\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f'blog/imgs/multi/{i}', bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General style transfer on a content image\n",
    "\n",
    "By using a single style image on a content image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to find images in the dataset\n",
    "\n",
    "fig, axes = plt.subplots(1, 5)\n",
    "\n",
    "for i, img in enumerate(data[940:][:5]):\n",
    "\taxes[i].imshow(data.invImg(img['img']))\n",
    "\taxes[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set content and style image features\n",
    "os.mkdir(f'blog/imgs/test')\n",
    "content_data = data[97]\n",
    "style_data = data[321]\n",
    "\n",
    "plt.imshow(data.invImg(content_data['img']))\n",
    "plt.title('Content image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print('attributes:', *[utils.get_attribute_name(att_id) for att_id in content_data['atts']], sep='\\n')\n",
    "\n",
    "plt.imshow(data.invImg(style_data['img']))\n",
    "plt.title('Style image')\n",
    "plt.axis('off')\n",
    "plt.savefig(f'blog/imgs/test/style', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('attributes:', *[utils.get_attribute_name(att_id) for att_id in style_data['atts']], sep='\\n')\n",
    "\n",
    "# save activations for content and style reference imgs\n",
    "model(content_data['img'].to(device))\n",
    "content_img_feature = activations['conv_4_2']\n",
    "\n",
    "model(style_data['img'].to(device))\n",
    "style_img_grams = {i: gram_matrix(activations[name]) for i, name in style_layers.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "ALPHA_BETA = 1e-3\n",
    "ITERATIONS = 10000\n",
    "LR = 1e-3\n",
    "\n",
    "# Set target image\n",
    "target_img = torch.tensor(content_data['img'], device=device, requires_grad=True)\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = torch.optim.Adam([target_img], lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import trange\n",
    "\n",
    "for i in trange(ITERATIONS + 1):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model(target_img)\n",
    "\n",
    "    target_content_feature = activations['conv_4_2']\n",
    "\n",
    "    content_loss = get_content_loss(content_img_feature, target_content_feature)\n",
    "\n",
    "    target_style_features = {k: activations[name] for k, name in style_layers.items()}\n",
    "\n",
    "    style_loss = get_style_loss(style_img_grams, target_style_features)\n",
    "\n",
    "    total_loss = ALPHA_BETA * content_loss + style_loss\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 5000 == 0:\n",
    "        plt.imshow(data.invImg(target_img.cpu().detach()))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Transfer, iteration: {i}, loss: {round(total_loss.item(), 4)}')\n",
    "        plt.savefig(f'blog/imgs/test/{i}', bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image, ImageDraw\n",
    "import IPython.display\n",
    "\n",
    "ann = utils.get_by_id('annotations', content_data['ann_id'])\n",
    "src_img = utils.get_by_id('images', ann['image_id'])\n",
    "file_name = src_img['file_name']\n",
    "\n",
    "x, y, width, height = map(int, ann['bbox'])\n",
    "\n",
    "res_img = T.Compose([\n",
    "    data.inv_normal,\n",
    "    T.Resize((height, width), antialias=True, interpolation=T.InterpolationMode.NEAREST),\n",
    "    lambda x: torch.clamp(x, 0, 1),\n",
    "    T.ToPILImage(),\n",
    "])(target_img)\n",
    "\n",
    "img = Image.open(f'fashionpedia/img/{file_name}').convert('RGB')\n",
    "\n",
    "mask = Image.new('1', img.size, 0)\n",
    "\n",
    "seg = ann['segmentation'][0]\n",
    "points = list(zip(*(iter(seg),) * 2))\n",
    "\n",
    "ImageDraw.Draw(mask).polygon(points, outline=1, fill=1)\n",
    "\n",
    "cropped_mask = mask.crop((x, y, x + width, y + height))\n",
    "\n",
    "res_img.putalpha(cropped_mask)\n",
    "\n",
    "img.paste(res_img, (x, y), res_img)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.savefig(f'blog/imgs/test/res', bbox_inches='tight')\n",
    "plt.show()\n",
    "IPython.display.Image(filename=f'fashionpedia/img/{file_name}', retina=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
