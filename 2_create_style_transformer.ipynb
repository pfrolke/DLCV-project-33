{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from models.vgg19style import VGG19Style\n",
    "from lib.fashionpedia_processed import FashionPediaProcessed\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = VGG19Style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = FashionPediaProcessed()\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(target, style, generated, alpha, beta):\n",
    "    return alpha * content_loss(target, generated) + beta * style_loss(style, generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "style_img = data[7]\n",
    "content_img = data[2]\n",
    "\n",
    "with open('fashionpedia/selected_attributes.json') as f:\n",
    "    s_att = list(json.load(f).values())\n",
    "\n",
    "print('attributes:', [s_att[i] for i, v in enumerate(content_img['att_oh']) if v])\n",
    "\n",
    "plt.imshow(data.invImg(content_img))\n",
    "plt.title('Content image')\n",
    "plt.show()\n",
    "\n",
    "print('attributes:', [s_att[i] for i, v in enumerate(style_img['att_oh']) if v])\n",
    "\n",
    "plt.imshow(data.invImg(style_img))\n",
    "plt.title('Style image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_img = target_item['img'].clone().requires_grad_(True)\n",
    "target_img = torch.normal(0, 1, content_img['img'].shape).requires_grad_(True)\n",
    "\n",
    "plt.imshow(target_img.detach().permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor: torch.Tensor):\n",
    "    depth = tensor.shape[0]\n",
    "    tensor = tensor.view(depth, -1)\n",
    "    return torch.mm(tensor, tensor.t()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "def set_activation(name):\n",
    "    return lambda _, __, output: activations.update({name: output})\n",
    "\n",
    "# content layer\n",
    "model.layers[21].register_forward_hook(set_activation('conv_4_2'))\n",
    "\n",
    "# style layers\n",
    "style_layers = {0: 'conv_1_1', 5: 'conv_2_1', 10: 'conv_3_1', 19: 'conv_4_1', 28: 'conv_5_1'}\n",
    "for i, name in style_layers.items():\n",
    "    model.layers[i].register_forward_hook(set_activation(name))\n",
    "\n",
    "model(content_img['img'])\n",
    "\n",
    "content_img_feature = activations['conv_4_2']\n",
    "\n",
    "model(style_img['img'])\n",
    "\n",
    "style_img_grams = {i: gram_matrix(activations[name]) for i, name in style_layers.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_content_loss(content_img_feature, target_img_feature):\n",
    "\treturn torch.sum((content_img_feature - target_img_feature) ** 2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "optimizer = torch.optim.Adam([target_img], lr=0.003)\n",
    "iterations = 3000\n",
    "\n",
    "for i in trange(1, iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model(target_img)\n",
    "\n",
    "    target_content_feature = activations['conv_4_2']\n",
    "\n",
    "    content_loss = get_content_loss(content_img_feature, target_content_feature)\n",
    "\n",
    "    content_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('loss:', content_loss.item())\n",
    "        plt.imshow(target_img.detach().permute(1,2,0))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(style_img_grams, target_img_features):\n",
    "\tloss = 0\n",
    "\n",
    "\tfor i in style_layers.keys():\n",
    "\t\ttarget_gram = gram_matrix(target_img_features[i])\n",
    "\t\tsquared_err = torch.sum((target_gram - style_img_grams[i]) ** 2)\n",
    "\n",
    "\t\t_, height, width = target_img_features[i].shape\n",
    "\t\tloss += squared_err / (4 * height ** 2 * width ** 2)\n",
    "\n",
    "\treturn loss / len(style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "optimizer = torch.optim.Adam([target_img], lr=0.003)\n",
    "iterations = 12000\n",
    "\n",
    "for i in trange(1, iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model(target_img)\n",
    "\n",
    "    target_style_features = {i: activations[name] for i, name in style_layers.items()}\n",
    "\n",
    "    style_loss = get_style_loss(style_img_grams, target_style_features)\n",
    "\n",
    "    style_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print('loss:', style_loss.item())\n",
    "        plt.imshow(target_img.detach().permute(1,2,0))\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
