{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from models.vgg19style import VGG19Style\n",
    "from lib.fashionpedia_processed import FashionPediaProcessed\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = VGG19Style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = FashionPediaProcessed()\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "style_data = data[7]\n",
    "content_data = data[2]\n",
    "\n",
    "with open('fashionpedia/selected_attributes.json') as f:\n",
    "    s_att = list(json.load(f).values())\n",
    "\n",
    "print('attributes:', [s_att[i] for i, v in enumerate(content_data['att_oh']) if v])\n",
    "\n",
    "plt.imshow(data.invImg(content_data))\n",
    "plt.title('Content image')\n",
    "plt.show()\n",
    "\n",
    "print('attributes:', [s_att[i] for i, v in enumerate(style_data['att_oh']) if v])\n",
    "\n",
    "plt.imshow(data.invImg(style_data))\n",
    "plt.title('Style image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_img = target_item['img'].clone().requires_grad_(True)\n",
    "noise_img = torch.normal(0, 1, content_data['img'].shape).requires_grad_(True)\n",
    "\n",
    "plt.imshow(noise_img.detach().permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor: torch.Tensor):\n",
    "    depth = tensor.shape[0]\n",
    "    tensor = tensor.view(depth, -1)\n",
    "    return torch.mm(tensor, tensor.t()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "def set_activation(name):\n",
    "    return lambda _, __, output: activations.update({name: output})\n",
    "\n",
    "# content layer\n",
    "model.layers[21].register_forward_hook(set_activation('conv_4_2'))\n",
    "\n",
    "# style layers\n",
    "style_layers = {0: 'conv_1_1', 5: 'conv_2_1', 10: 'conv_3_1', 19: 'conv_4_1', 28: 'conv_5_1'}\n",
    "for i, name in style_layers.items():\n",
    "    model.layers[i].register_forward_hook(set_activation(name))\n",
    "\n",
    "model(content_data['img'])\n",
    "\n",
    "content_img_feature = activations['conv_4_2']\n",
    "\n",
    "model(style_data['img'])\n",
    "\n",
    "style_img_grams = {i: gram_matrix(activations[name]) for i, name in style_layers.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single image content transfer from noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_content_loss(content_img_feature, target_img_feature):\n",
    "\treturn torch.sum((content_img_feature - target_img_feature) ** 2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "optimizer = torch.optim.Adam([target_img], lr=0.003)\n",
    "iterations = 3000\n",
    "\n",
    "target_img = noise_img\n",
    "\n",
    "for i in trange(1, iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model(target_img)\n",
    "\n",
    "    target_content_feature = activations['conv_4_2']\n",
    "\n",
    "    content_loss = get_content_loss(content_img_feature, target_content_feature)\n",
    "\n",
    "    content_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('loss:', content_loss.item())\n",
    "        plt.imshow(target_img.detach().permute(1,2,0))\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single image style transfer from noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(style_img_grams, target_img_features):\n",
    "\tloss = 0\n",
    "\n",
    "\tfor i in style_layers.keys():\n",
    "\t\ttarget_gram = gram_matrix(target_img_features[i])\n",
    "\t\tsquared_err = torch.sum((target_gram - style_img_grams[i]) ** 2)\n",
    "\n",
    "\t\t_, height, width = target_img_features[i].shape\n",
    "\t\tloss += squared_err / (4 * height ** 2 * width ** 2)\n",
    "\n",
    "\treturn loss / len(style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "optimizer = torch.optim.Adam([target_img], lr=0.003)\n",
    "iterations = 12000\n",
    "\n",
    "target_img = noise_img\n",
    "\n",
    "for i in trange(1, iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model(target_img)\n",
    "\n",
    "    target_style_features = {i: activations[name] for i, name in style_layers.items()}\n",
    "\n",
    "    style_loss = get_style_loss(style_img_grams, target_style_features)\n",
    "\n",
    "    style_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print('loss:', style_loss.item())\n",
    "        plt.imshow(target_img.detach().permute(1,2,0))\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General style transfer on a content image\n",
    "\n",
    "By using multiple images with the same style might generalize a style on the content image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all images with certain style \n",
    "floral_i = 30\n",
    "checker_i = 29\n",
    "stripe_i = 31\n",
    "\n",
    "style_att_id = checker_i\n",
    "\n",
    "style_images = []\n",
    "for i, d in enumerate(data): \n",
    "\tif d['att_oh'][style_att_id]:\n",
    "\t\tstyle_images.append(d['img'])\n",
    "\n",
    "len_style_images = len(style_images)\n",
    "print(f'Number of style images: {len_style_images}')\n",
    "plt.imshow(data.invImg({'img': style_images[3]}))\n",
    "plt.title(f'Style image: {s_att[style_att_id]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set content and style image features\n",
    "content_image = content_data['img']\n",
    "\n",
    "model(content_image)\n",
    "\n",
    "content_img_feature = activations['conv_4_2']\n",
    "\n",
    "style_images_grams = []\n",
    "\n",
    "for style_image in tqdm(style_images):\n",
    "\tmodel(style_image)\n",
    "\n",
    "\tstyle_images_grams.append({i: gram_matrix(activations[name]) for i, name in style_layers.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "ALPHA = 1\n",
    "BETA = 1e2\n",
    "SWITCH_STYLE_IMG_EVERY = 5\n",
    "ITERATIONS = 12000\n",
    "LR = 3e-3\n",
    "\n",
    "target_img = content_image\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = torch.optim.Adam([target_img], lr=LR)\n",
    "\n",
    "\n",
    "for i in trange(1, ITERATIONS // SWITCH_STYLE_IMG_EVERY):\n",
    "    style_img_grams = style_images_grams[i % len_style_images]\n",
    "\n",
    "    for j in range(SWITCH_STYLE_IMG_EVERY):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model(target_img)\n",
    "\n",
    "        target_content_feature = activations['conv_4_2']\n",
    "\n",
    "        content_loss = get_content_loss(content_img_feature, target_content_feature)\n",
    "\n",
    "        target_style_features = {k: activations[name] for k, name in style_layers.items()}\n",
    "\n",
    "        style_loss = get_style_loss(style_img_grams, target_style_features)\n",
    "\n",
    "        total_loss = ALPHA * content_loss + BETA * style_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('loss:', total_loss.item())\n",
    "        plt.imshow(data.invImg({'img': target_img.detach()}))\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General style transfer on a content image\n",
    "\n",
    "By using a single style image on a content image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set content and style image features\n",
    "content_img = content_data['img']\n",
    "style_image = style_images[3]\n",
    "\n",
    "model(content_img)\n",
    "\n",
    "content_img_feature = activations['conv_4_2']\n",
    "\n",
    "model(style_image)\n",
    "\n",
    "style_img_grams = {i: gram_matrix(activations[name]) for i, name in style_layers.items()}\n",
    "\n",
    "plt.imshow(data.invImg({'img': content_img}))\n",
    "plt.title(\"Content Image\")\n",
    "plt.show()\n",
    "plt.imshow(data.invImg({'img': style_image}))\n",
    "plt.title(\"Style Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "ALPHA = 1\n",
    "BETA = 1e2\n",
    "ITERATIONS = 12000\n",
    "LR = 3e-3\n",
    "\n",
    "# Set target image\n",
    "target_img = content_img\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = torch.optim.Adam([target_img], lr=LR)\n",
    "\n",
    "\n",
    "for i in trange(1, ITERATIONS // SWITCH_STYLE_IMG_EVERY):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model(target_img)\n",
    "\n",
    "    target_content_feature = activations['conv_4_2']\n",
    "\n",
    "    content_loss = get_content_loss(content_img_feature, target_content_feature)\n",
    "\n",
    "    target_style_features = {k: activations[name] for k, name in style_layers.items()}\n",
    "\n",
    "    style_loss = get_style_loss(style_img_grams, target_style_features)\n",
    "\n",
    "    total_loss = ALPHA * content_loss + BETA * style_loss\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('loss:', total_loss.item())\n",
    "        img = target_img.detach()\n",
    "        plt.imshow(data.invImg({'img': img}))\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
